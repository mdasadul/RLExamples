{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np \n",
    "import tensorflow as tf \n",
    "\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(object):\n",
    "    def __init__(self, n_feature, n_action,sess,learning_rate = 0.01):\n",
    "        self.sess = sess\n",
    "        self.state = tf.placeholder(tf.float32, [1,n_feature],name='state')\n",
    "        self.action = tf.placeholder(tf.int32,None, name='action')\n",
    "        self.td_error = tf.placeholder(tf.float32, None, name='td_error')\n",
    "\n",
    "\n",
    "        with tf.variable_scope('actor'):\n",
    "            l1 = tf.layers.dense(self.state,\n",
    "                                20,\n",
    "                                activation=tf.nn.relu,\n",
    "                                kernel_initializer=tf.random_normal_initializer(0.,0.1),\n",
    "                                bias_initializer=tf.constant_initializer(0.1),\n",
    "                                name='l1')\n",
    "            self.ac_prob = tf.layers.dense( l1,\n",
    "                                        n_action,\n",
    "                                        activation=tf.nn.softmax,\n",
    "                                        kernel_initializer=tf.random_normal_initializer(0.,0.1),\n",
    "                                        bias_initializer = tf.constant_initializer(0.1),\n",
    "                                        name = 'activation_prob')\n",
    "        \n",
    "        with tf.variable_scope('td_error'):\n",
    "            log_prob = tf.log(self.ac_prob[0,self.action])\n",
    "            self.exp_v = tf.reduce_mean(self.td_error*log_prob)\n",
    "        \n",
    "        with tf.variable_scope('training_op'):\n",
    "            self.train_op = tf.train.AdamOptimizer(learning_rate).minimize(-self.exp_v)\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        #state = state[np.newaxis,:]\n",
    "        ac_prob = self.sess.run(self.ac_prob, feed_dict = {self.state:state})\n",
    "        return np.argmax(ac_prob,1)[0]#np.random.choice(np.arange(ac_prob.shape[1]), p=ac_prob.ravel())\n",
    "    \n",
    "\n",
    "\n",
    "    def learn(self, state, action,error):\n",
    "        #state = state[np.newaxis,:]\n",
    "        feeddict = {self.state: state, self.action:action,self.td_error:error}\n",
    "\n",
    "        exp_v,_ = self.sess.run([self.exp_v,self.train_op], feed_dict = feeddict)\n",
    "        return exp_v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "print(np.argmax(np.random.rand(1,4),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Critic(object):\n",
    "    def __init__(self, n_feature, sess, learning_rate, gamma = 0.9):\n",
    "        self.sess = sess\n",
    "        self.state = tf.placeholder(tf.float32, [1,n_feature], name='state')\n",
    "        self.reward = tf.placeholder(tf.float32, None, name = 'reward')\n",
    "        self.v_ = tf.placeholder(tf.float32, [1,1], name = 'v')\n",
    "\n",
    "        with tf.variable_scope('critics'):\n",
    "            l1 = tf.layers.dense(self.state,\n",
    "                                20,\n",
    "                                activation=tf.nn.relu,\n",
    "                                kernel_initializer=tf.random_normal_initializer(0.,0.1),\n",
    "                                bias_initializer=tf.constant_initializer(0.1),\n",
    "                                name='l1')\n",
    "            self.v = tf.layers.dense( l1,\n",
    "                                        1,\n",
    "                                        activation=None,\n",
    "                                        kernel_initializer=tf.random_normal_initializer(0.,0.1),                                        bias_initializer = tf.constant_initializer(0.1),\n",
    "                                        name = 'activation_prob')\n",
    "\n",
    "            with tf.variable_scope('tf_error'):\n",
    "                self.td_error = self.reward + gamma * self.v_ - self.v\n",
    "                self.loss = tf.square(self.td_error)\n",
    "            with tf.variable_scope('train_op'):\n",
    "                self.train_op = tf.train.AdamOptimizer(learning_rate).minimize(self.loss)\n",
    "            \n",
    "    def learn(self, state, reward,state_):\n",
    "        #state,state_ = state[np.newaxis,:],state_[np.newaxis,:]\n",
    "\n",
    "        exp_v = self.sess.run(self.v, feed_dict={self.state:state_})\n",
    "\n",
    "        feeddict = {self.reward: reward, self.v_:exp_v, self.state: state}\n",
    "        _, error=self.sess.run([self.train_op,self.td_error],feeddict)\n",
    "\n",
    "        return error\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'numpy.ndarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-6e50097983bc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[1;31m#env.render()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mactor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoose_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meye\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_feature\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m             \u001b[0mstate_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[0mtd_error\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcritic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meye\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_feature\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meye\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_feature\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate_\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstate_\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\em1052\\appdata\\local\\continuum\\miniconda3\\lib\\site-packages\\gym\\wrappers\\time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_episode_started_at\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\em1052\\appdata\\local\\continuum\\miniconda3\\lib\\site-packages\\gym\\envs\\toy_text\\discrete.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, a)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m         \u001b[0mtransitions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mP\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m         \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcategorical_sample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtransitions\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mtransitions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
     ]
    }
   ],
   "source": [
    "if __name__ =='__main__':\n",
    "    env = gym.make('Taxi-v2')\n",
    "    n_feature = env.observation_space.n\n",
    "    n_action = env.action_space.n\n",
    "    tf.reset_default_graph()\n",
    "    sess = tf.Session()\n",
    "   \n",
    "    actor = Actor(n_feature, n_action,sess,learning_rate= 0.001)\n",
    "    critic = Critic(n_feature,sess,learning_rate=0.01)\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    num_tests = 100\n",
    "\n",
    "    reward_list =[]\n",
    "    jlist = []\n",
    "    for t in range(num_tests):\n",
    "        state = env.reset()\n",
    "        \n",
    "        rewards = 0\n",
    "        while True:\n",
    "            #if t>100:\n",
    "            #env.render()\n",
    "            action = actor.choose_action(np.eye(n_feature)[state:state+1])\n",
    "            state_, reward, done, _ = env.step(action)\n",
    "            \n",
    "            td_error = critic.learn(np.eye(n_feature)[state:state+1],reward,np.eye(n_feature)[state_:state_+1])\n",
    "            actor.learn(np.eye(n_feature)[state:state+1], action, td_error)\n",
    "            state = state_\n",
    "        \n",
    "            rewards += reward\n",
    "            print(reward)\n",
    "            if done:\n",
    "                \n",
    "                print('Total rewards: %d' %(rewards))\n",
    "                break\n",
    "        \n",
    "        reward_list.append(rewards)\n",
    "        #jlist.append(j)\n",
    "    \n",
    "    print(np.sum(reward_list)/num_tests)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
